<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanshu Zhang</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <link href="https://fonts.cdnfonts.com/css/optima" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
  <link rel="icon" type="image/png" href="images/icon.png">

</head>

<body>
    <div class="container_onecolumn">
        <div class="container">
            <div class="item text">
              <p>
                <name>Yanshu Zhang</name>
              </p>
              <br>
              <!-- <br> -->
              <p>Hi there! I am a Ph.D. student in <a href="https://sfuapex.com/">APEX Lab</a> 
                at <a href="https://gruvi.cs.sfu.ca/">Simon Fraser University</a>, 
                supervised by <a href="https://www.sfu.ca/~keli/">Ke Li</a>. 
                <!-- <br> -->
                Prior to that, I received my Bachelor's degree in computer science from 
                <a href="https://en.cs.ustc.edu.cn/main.htm">University of Science and Technology of China</a>.
              </p>
              <p>
              My research focuses on neural rendering and its applications.
              </p> 
              <p>
                <a href="mailto:yszhang170@gmail.com"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=JMvdDZwAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yszhang170"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/zvict/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
                
              </p>
            </div>
 
            <div class="item image">
              <!-- <img class="profile-photo" alt="profile photo" src="images/me.jpg"> -->
              <a href="game_of_life.html" title="this gonna waste u 5min", target="_blank">
                <img src="images/me.jpg" alt="Image">
              </a>
            </div>
          </div>
          
          
            <div class="item text2">
            <heading>Publications</heading>
            </div>

            <div class="container2">
              <div class="item image2">
                  <img src='images/pim-lego-crop.gif' width=1800; height="auto">
              </div>
              <div class="item text2">
              <a href="https://niopeng.github.io/PAPR-in-Motion/">
                  <font color=#1772d0>  <papertitle>PAPR in Motion: Seamless Point-level 3D Scene Interpolation</papertitle></font>
              </a>
              <br>
              <a href=https://sites.google.com/view/niopeng/home>Shichong Peng</a>, 
              <strong>Yanshu Zhang</strong>, 
              <a href=https://www.sfu.ca/~keli>Ke Li</a>
              <br>
              <em>CVPR</em>, 2024 (Highlight)
              <br>
              <a href="https://niopeng.github.io/PAPR-in-Motion/">Project Page</a> /
              <a href="https://github.com/niopeng/PAPR-in-Motion">Code</a> /
              <a href="https://arxiv.org/abs/2406.05533">arXiv</a> /
              <a href="https://youtu.be/vysmn3TN4FY">Video</a> /
<button id="bib_button" class="link", onclick="showBib('PIM_bib')">BibTex</button>
<div id='PIM_bib' hidden>
<pre><code>@inproceedings{peng2024papr,
  title={PAPR in Motion: Seamless Point-level 3D Scene Interpolation},
  author={Shichong Peng and Yanshu Zhang and Ke Li},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}</code></pre>
</div>        
            <p></p>
            <p>We introduce the novel problem of point-level 3D scene interpolation. 
              Given observations of a scene at two distinct states from multiple views, 
              the goal is to synthesize a smooth point-level interpolation between them, 
              without any intermediate supervision. Our method, PAPR in Motion, builds 
              upon Proximity Attention Point Rendering (PAPR) technique, and generates 
              seamless interpolations of both the scene geometry and appearance.
            </p> 
          </div>
        </div>

        <hr>

            <div class="container2">
                <div class="item image2">
                    <img src='images/papr-crop.gif' width=180; height="auto">
                </div>
                <div class="item text2">
                <a href="https://zvict.github.io/papr/">
                    <font color=#1772d0>  <papertitle>PAPR: Proximity Attention Point Rendering</papertitle></font>
                </a>
                <br>
                <strong>Yanshu Zhang*</strong>, 
                <a href=https://sites.google.com/view/niopeng/home>Shichong Peng*</a>, 
                <a href=https://amoazeni75.github.io>Alireza Moazeni</a>,
                <a href=https://www.sfu.ca/~keli>Ke Li</a>
                <br>
                <em>NeurIPS</em>, 2023 (Spotlight)
                <br>
                <a href="https://zvict.github.io/papr/">Project Page</a> /
                <a href="https://github.com/zvict/papr">Code</a> /
                <a href="https://arxiv.org/abs/2307.11086">arXiv</a> /
                <a href="https://youtu.be/1atBGH_pDHY">Video</a> /
<button id="bib_button" class="link", onclick="showBib('PAPR_bib')">BibTex</button>
<div id='PAPR_bib' hidden>
<pre><code>@inproceedings{zhang2023papr,
  title={PAPR: Proximity Attention Point Rendering},
  author={Yanshu Zhang and Shichong Peng and Alireza Moazeni and Ke Li},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}</code></pre>
</div>        
              <p></p>
              <p>PAPR is a point-based surface representation that uses proximity attention to 
                interpolate between nearby points to rays for rendering high-quality images, 
                enabling non-volume-preserving geometry deformation by directly adjusting point 
                positions, and, unlike 3D Gaussian Splatting, it avoids creating holes while 
                preserving texture details after deformation.
              </p> 
            </div>
          </div>

        <!-- <hr> -->

        <!-- <div class="item text2">
          <p>
            <heading>Academic Services</heading>
          </p>
           <p>
          <ul>
              <li>Served as reviewer for NeurIPS, ICLR, CVPR, and SIGGRAPH.</li>
          </ul>
          </p>
      </div> -->


      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;">
                 Template adapted from <a href="https://qianlim.github.io/">Qianli Ma</a>'s websites.
              </p>
            </td>
          </tr>
        </tbody></table>

<script type="text/javascript" src="show_bib.js"></script>
</body>
</html>
